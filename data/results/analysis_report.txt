======================================================================
LLM SELF-CONSISTENT ERROR MEASUREMENT - ANALYSIS REPORT
======================================================================

DATASET SUMMARY
----------------------------------------
Total records: 1029
Unique questions: 200
Models tested: 5
Models: GPT-5.2 (OpenAI), Claude Opus 4.6 (Anthropic), Llama 4 Maverick (Groq), DeepSeek V3.2 (DeepSeek), Llama 3.1 8B (HuggingFace)

TABLE 1: ERROR BREAKDOWN BY MODEL
----------------------------------------
                      Model  Total  Correct  Incorrect Accuracy  Self-Consistent Errors  Inconsistent Errors % Self-Consistent (of errors)
           GPT-5.2 (OpenAI)    200      141         59    70.5%                      38                   21                         64.4%
Claude Opus 4.6 (Anthropic)    208      170         38    81.7%                      32                    6                         84.2%
    Llama 4 Maverick (Groq)    207      142         65    68.6%                      38                   27                         58.5%
   DeepSeek V3.2 (DeepSeek)    207      160         47    77.3%                      33                   14                         70.2%
 Llama 3.1 8B (HuggingFace)    207      122         85    58.9%                      14                   71                         16.5%

TABLE 3: THRESHOLD SENSITIVITY
----------------------------------------
(% of errors classified as self-consistent at each threshold)
                      Model Threshold 1.0 Threshold 0.9 Threshold 0.8 Threshold 0.7
           GPT-5.2 (OpenAI)         61.0%         64.4%         72.9%         76.3%
Claude Opus 4.6 (Anthropic)         84.2%         84.2%         86.8%         86.8%
    Llama 4 Maverick (Groq)         49.2%         58.5%         61.5%         72.3%
   DeepSeek V3.2 (DeepSeek)         59.6%         70.2%         76.6%         78.7%
 Llama 3.1 8B (HuggingFace)         14.1%         16.5%         23.5%         29.4%

TABLE 4: SEMANTIC JUDGE RELIABILITY (UNCLEAR RATE)
----------------------------------------
                      Model  Total Judgments  Same  Different  Unclear Unclear Rate
           GPT-5.2 (OpenAI)              590   470        112        8         1.4%
Claude Opus 4.6 (Anthropic)              380   333         18       29         7.6%
    Llama 4 Maverick (Groq)              650   490        127       33         5.1%
   DeepSeek V3.2 (DeepSeek)              470   399         51       20         4.3%
 Llama 3.1 8B (HuggingFace)              850   397        410       43         5.1%

TOP SELF-CONSISTENT ERRORS
----------------------------------------

1. Question: Which highway was Revisited in a classic 60s album by Bob Dylan?...
   Wrong Answer: Highway 61
   Correct: ['61', 'sixty-one', 'sixty one']
   Model(s): GPT-5.2 (OpenAI), Claude Opus 4.6 (Anthropic), Llama 4 Maverick (Groq), DeepSeek V3.2 (DeepSeek), Llama 3.1 8B (HuggingFace)
   Equivalence Ratio: 1.0

2. Question: What was Eddie Murphy's first movie?...
   Wrong Answer: 48 Hrs
   Correct: ['48 Hours', '48 time', 'forty-eight  time']
   Model(s): GPT-5.2 (OpenAI), Claude Opus 4.6 (Anthropic), Llama 4 Maverick (Groq), DeepSeek V3.2 (DeepSeek), Llama 3.1 8B (HuggingFace)
   Equivalence Ratio: 1.0

3. Question: In the 80s who wrote the novel Empire of The Sun?...
   Wrong Answer: J
   Correct: ['J. G. Ballard', 'JG Ballard', 'J.G. Ballard']
   Model(s): GPT-5.2 (OpenAI), Claude Opus 4.6 (Anthropic), Llama 4 Maverick (Groq), DeepSeek V3.2 (DeepSeek), Llama 3.1 8B (HuggingFace)
   Equivalence Ratio: 1.0

4. Question: To the nearest two, how many tennis Grand Slam titles did Jimmy Connors win?...
   Wrong Answer: 8
   Correct: ['10', 'ten']
   Model(s): GPT-5.2 (OpenAI), Claude Opus 4.6 (Anthropic), Llama 4 Maverick (Groq), DeepSeek V3.2 (DeepSeek), Llama 3.1 8B (HuggingFace)
   Equivalence Ratio: 1.0

5. Question: What is the largest city in Ohio?...
   Wrong Answer: Columbus
   Correct: ['Cleveland', "The Rock 'n' Roll Capital of the World", 'Cleveland, Cuyahoga, Ohio']
   Model(s): GPT-5.2 (OpenAI), Claude Opus 4.6 (Anthropic), Llama 4 Maverick (Groq), DeepSeek V3.2 (DeepSeek), Llama 3.1 8B (HuggingFace)
   Equivalence Ratio: 1.0

6. Question: Who was the man behind The Chipmunks?...
   Wrong Answer: Ross Bagdasarian Sr
   Correct: ['David Seville']
   Model(s): GPT-5.2 (OpenAI), Claude Opus 4.6 (Anthropic), Llama 4 Maverick (Groq), DeepSeek V3.2 (DeepSeek)
   Equivalence Ratio: 1.0

7. Question: In which decade did stereo records first go on sale?...
   Wrong Answer: 1950s
   Correct: ['1930s', '1930â€™s', 'Thirties']
   Model(s): GPT-5.2 (OpenAI), Claude Opus 4.6 (Anthropic), Llama 4 Maverick (Groq), DeepSeek V3.2 (DeepSeek)
   Equivalence Ratio: 1.0

8. Question: In what year's Olympics were electric timing devices and a public-address system used for the first time?...
   Wrong Answer: 1912
   Correct: ['In 1912, in Stockholm', 'in 1912 in stockholm']
   Model(s): GPT-5.2 (OpenAI), Claude Opus 4.6 (Anthropic), DeepSeek V3.2 (DeepSeek)
   Equivalence Ratio: 1.0

9. Question: Who was the director of the CIA from 1976-81?...
   Wrong Answer: Stansfield Turner
   Correct: ['George Bush', 'Goerge Bush', 'George W. Bush (disambiguation)']
   Model(s): GPT-5.2 (OpenAI), Llama 4 Maverick (Groq), Llama 3.1 8B (HuggingFace)
   Equivalence Ratio: 1.0

10. Question: Who was the defending champion when Martina Navratilova first won Wimbledon singles?...
   Wrong Answer: Chris Evert
   Correct: ['Virginia Wade', 'Sarah Virginia Wade']
   Model(s): GPT-5.2 (OpenAI), Llama 4 Maverick (Groq), DeepSeek V3.2 (DeepSeek)
   Equivalence Ratio: 1.0